[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nXingyu Kuang\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "1v0site",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe control group received a standard letter, while the treatment groups received letters offering matching grants with varying features. These included different match ratios (1:1, 2:1, and 3:1), varying maximum matching amounts ($25,000, $50,000, $100,000, or unspecified), and suggested donation levels tied to past giving. The study aimed to isolate whether the “price” of giving—lowered through matching—would increase donation likelihood or size. Results showed that simply including a matching grant significantly increased response rates and revenue per letter. However, larger match ratios offered no additional benefit. The experiment provided strong empirical insight into donor behavior and the psychology behind charitable decision-making.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe control group received a standard letter, while the treatment groups received letters offering matching grants with varying features. These included different match ratios (1:1, 2:1, and 3:1), varying maximum matching amounts ($25,000, $50,000, $100,000, or unspecified), and suggested donation levels tied to past giving. The study aimed to isolate whether the “price” of giving—lowered through matching—would increase donation likelihood or size. Results showed that simply including a matching grant significantly increased response rates and revenue per letter. However, larger match ratios offered no additional benefit. The experiment provided strong empirical insight into donor behavior and the psychology behind charitable decision-making.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\nThe dataset contains 50,083 observations and 51 variables from a natural field experiment on charitable giving. Each row represents a prior donor who received a fundraising letter with randomized treatments: a control letter or one of several matching grant variations. Key variables include match ratio (ratio), maximum match size (size), suggested donation amounts (ask), and donation behavior (gave, amount). Demographic and geographic data—like gender, income, urban status, and political affiliation—are also included. The dataset enables analysis of how different fundraising strategies affect donation likelihood and size, offering insights into behavioral economics and nonprofit fundraising effectiveness.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport numpy as np\nimport statsmodels.formula.api as smf\n\ndef test_balance(variable: str):\n    # Drop missing values\n    treat = df[df['treatment'] == 1][variable].dropna()\n    ctrl = df[df['treatment'] == 0][variable].dropna()\n\n    X1, X2 = treat.mean(), ctrl.mean()\n    s1, s2 = treat.std(), ctrl.std()\n    n1, n2 = len(treat), len(ctrl)\n\n    # Manual t-statistic using class slide formula\n    t_stat_manual = (X1 - X2) / np.sqrt((s1**2)/n1 + (s2**2)/n2)\n\n    reg = smf.ols(f'{variable} ~ treatment', data=df[['treatment', variable]].dropna()).fit()\n    coef = reg.params['treatment']\n    t_stat_reg = reg.tvalues['treatment']\n    p_val = reg.pvalues['treatment']\n\n    return {\n        \"variable\": variable,\n        \"manual t-stat\": round(t_stat_manual, 4),\n        \"regression coef\": round(coef, 4),\n        \"regression t-stat\": round(t_stat_reg, 4),\n        \"p-value\": round(p_val, 4),\n        \"significant at 95%\": p_val &lt; 0.05\n    }\n\nall_vars_to_test = [\n    'mrm2', 'years', 'female', 'couple',     # behavioral/demographic\n    'hpa', 'ltmedmra', 'freq',               # prior donation history\n    'pwhite', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba'  # ZIP-level demographics\n]\n\nresults = [test_balance(var) for var in all_vars_to_test]\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\n\n\n\n\n\nvariable\nmanual t-stat\nregression coef\nregression t-stat\np-value\nsignificant at 95%\n\n\n\n\n0\nmrm2\n0.1195\n0.0137\n0.1195\n0.9049\nFalse\n\n\n1\nyears\n-1.0909\n-0.0575\n-1.1030\n0.2700\nFalse\n\n\n2\nfemale\n-1.7535\n-0.0075\n-1.7584\n0.0787\nFalse\n\n\n3\ncouple\n-0.5823\n-0.0016\n-0.5838\n0.5594\nFalse\n\n\n4\nhpa\n0.9704\n0.6371\n0.9441\n0.3451\nFalse\n\n\n5\nltmedmra\n1.9099\n0.0091\n1.9097\n0.0562\nFalse\n\n\n6\nfreq\n-0.1108\n-0.0120\n-0.1109\n0.9117\nFalse\n\n\n7\npwhite\n-0.5590\n-0.0009\n-0.5603\n0.5753\nFalse\n\n\n8\nave_hh_sz\n0.8234\n0.0030\n0.8243\n0.4098\nFalse\n\n\n9\nmedian_hhincome\n-0.7433\n-157.9255\n-0.7417\n0.4583\nFalse\n\n\n10\npowner\n0.1895\n0.0004\n0.1891\n0.8500\nFalse\n\n\n11\npsch_atlstba\n-1.8427\n-0.0033\n-1.8481\n0.0646\nFalse\n\n\n\n\n\n\n\nWe conducted a series of balance tests on baseline covariates using both manual t-tests and linear regression. The t-tests follow the classical formula, where the difference in sample means between the treatment and control groups is scaled by the standard error of that difference. This standardization produces a t-statistic, which tells us how extreme the observed difference is under the assumption that there is no true difference.For each variable tested—such as months since last donation (mrm2), years since initial donation (years), whether the last gift was below the median (ltmedmra), and demographic variables like gender (female)—we calculated the t-statistic, the corresponding p-value, and the regression-based estimate of group differences.Across all variables, the p-values exceeded 0.05, indicating no statistically significant differences between the treatment and control groups at the 95% confidence level. This strongly suggests that the random assignment was successful, and that the groups are balanced on observed characteristics.This aligns with Table 1 in the original paper, which serves to reassure readers that the treatment effects observed later in the study can be attributed to the intervention itself, not to any pre-existing differences between groups. These balance checks are crucial in experimental work because they support the assumption that any observed differences in outcomes are causal, not confounded by selection bias or unbalanced covariates. In short, the t-test results confirm that the randomization worked as intended and that the internal validity of the study is strong."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\ndonation_rate = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\ndonation_rate.plot(kind='bar', edgecolor='black')\nplt.title(\"Proportion of People Who Donated by Group\")\nplt.ylabel(\"Donation Rate\")\nplt.xticks([0, 1], ['Control', 'Treatment'], rotation=0)\nplt.ylim(0, donation_rate.max() + 0.02)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_ctrl = df[df['treatment'] == 0]['gave']\n\nt_stat, p_val = ttest_ind(gave_treat, gave_ctrl, equal_var=False)\n\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nresults = pd.DataFrame({\n    \"Method\": [\"T-test\", \"OLS Regression\"],\n    \"Estimate\": [None, round(reg_gave.params['treatment'], 4)],\n    \"t-statistic\": [round(t_stat, 4), round(reg_gave.tvalues['treatment'], 4)],\n    \"p-value\": [round(p_val, 4), round(reg_gave.pvalues['treatment'], 4)],\n    \"R-squared\": [None, round(reg_gave.rsquared, 4)]\n})\n\nresults\n\n\n\n\n\n\n\n\nMethod\nEstimate\nt-statistic\np-value\nR-squared\n\n\n\n\n0\nT-test\nNaN\n3.2095\n0.0013\nNaN\n\n\n1\nOLS Regression\n0.0042\n3.1014\n0.0019\n0.0002\n\n\n\n\n\n\n\nThe results of both the t-test and regression show that individuals who received a matching donation offer were significantly more likely to donate than those in the control group. While the increase in donation rate is modest, the statistical evidence suggests it is unlikely due to chance. This highlights that even small behavioral nudges—like framing a donation as part of a matching gift—can meaningfully affect charitable behavior. People may perceive matched gifts as a way to increase their personal impact, which boosts their motivation to give. The findings support the idea that well-designed incentives can effectively increase donor participation.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Run probit regression: gave ~ treatment\nprobit_model = sm.Probit(df['gave'], sm.add_constant(df['treatment']))\nprobit_result = probit_model.fit(disp=0)\n\n# Format result as a table\nprobit_table = pd.DataFrame({\n    \"Metric\": [\n        \"Coefficient (treatment)\", \n        \"Standard Error\", \n        \"z-stat\", \n        \"p-value\", \n        \"Pseudo R-squared\", \n        \"Number of Observations\"\n    ],\n    \"Value\": [\n        round(probit_result.params['treatment'], 4),\n        round(probit_result.bse['treatment'], 4),\n        round(probit_result.tvalues['treatment'], 4),\n        round(probit_result.pvalues['treatment'], 4),\n        round(probit_result.prsquared, 4),\n        int(probit_result.nobs)\n    ]\n})\n\nprobit_table\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nCoefficient (treatment)\n0.0868\n\n\n1\nStandard Error\n0.0279\n\n\n2\nz-stat\n3.1129\n\n\n3\np-value\n0.0019\n\n\n4\nPseudo R-squared\n0.0010\n\n\n5\nNumber of Observations\n50083.0000\n\n\n\n\n\n\n\nTo replicate Column 1 of Table 3 in Karlan and List (2007), we estimated a probit regression where the binary outcome variable is whether an individual made a charitable donation (gave), and the explanatory variable is assignment to the treatment group (treatment).\nThe regression output shows a statistically significant positive effect of the treatment on the probability of donating. The estimated coefficient for treatment is 0.087, with a standard error of 0.028, yielding a z-statistic of 3.11 and a p-value of 0.0019. The pseudo R-squared is 0.001, and the number of observations is 50,083—exactly matching the paper.\nThis result reinforces the paper’s main finding: matching offers causally increase the likelihood of donating. Even though the absolute change is small, it’s meaningful in the context of low baseline donation rates, and it highlights how subtle changes in framing can influence real-world giving behavior.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\ntreat_df = df[df['treatment'] == 1]\n\ngave_1to1 = treat_df[treat_df['ratio'] == 1]['gave']\ngave_2to1 = treat_df[treat_df['ratio'] == 2]['gave']\ngave_3to1 = treat_df[treat_df['ratio'] == 3]['gave']\n\nttest_2v1 = ttest_ind(gave_2to1, gave_1to1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3to1, gave_1to1, equal_var=False)\nttest_3v2 = ttest_ind(gave_3to1, gave_2to1, equal_var=False)\n\nt_test_results = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 1:1\", \"3:1 vs 2:1\"],\n    \"t-statistic\": [\n        round(ttest_2v1.statistic, 4),\n        round(ttest_3v1.statistic, 4),\n        round(ttest_3v2.statistic, 4)\n    ],\n    \"p-value\": [\n        round(ttest_2v1.pvalue, 4),\n        round(ttest_3v1.pvalue, 4),\n        round(ttest_3v2.pvalue, 4)\n    ]\n})\n\nt_test_results\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\n\n\n\n\n0\n2:1 vs 1:1\n0.9650\n0.3345\n\n\n1\n3:1 vs 1:1\n1.0150\n0.3101\n\n\n2\n3:1 vs 2:1\n0.0501\n0.9600\n\n\n\n\n\n\n\nTo evaluate whether larger match ratios (2:1 or 3:1) increase the likelihood of donating compared to a 1:1 match, we conducted a series of t-tests using only individuals in the treatment group.\nThe results showed no statistically significant differences in donation rates between: * 2:1 and 1:1 match groups * 3:1 and 1:1 match groups * 3:1 and 2:1 match groups\nThese findings suggest that increasing the generosity of the match offer does not significantly affect whether people donate. This directly supports the authors’ comment in the paper (page 8), where they note that the match ratio had no additional effect on donor behavior beyond the presence of a match itself.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ntreatment_df = df[df['treatment'] == 1].copy()\n\ntreatment_df['ratio1'] = (treatment_df['ratio'] == 1).astype(int)\ntreatment_df['ratio2'] = (treatment_df['ratio'] == 2).astype(int)\ntreatment_df['ratio3'] = (treatment_df['ratio'] == 3).astype(int)\n\nreg = smf.ols('gave ~ ratio2 + ratio3', data=treatment_df).fit()\n\nols_table = pd.DataFrame({\n    \"Variable\": reg.params.index,\n    \"Coefficient\": reg.params.round(4).values,\n    \"Std. Error\": reg.bse.round(4).values,\n    \"t-stat\": reg.tvalues.round(4).values,\n    \"p-value\": reg.pvalues.round(4).values\n})\n\nols_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-stat\np-value\n\n\n\n\n0\nIntercept\n0.0207\n0.0014\n14.9122\n0.0000\n\n\n1\nratio2\n0.0019\n0.0020\n0.9576\n0.3383\n\n\n2\nratio3\n0.0020\n0.0020\n1.0083\n0.3133\n\n\n\n\n\n\n\nTo assess whether higher match ratios (2:1 or 3:1) are more effective than a 1:1 match at encouraging donations, we ran a linear regression using data from individuals in the treatment group only.\nThese results suggest that larger match ratios do not significantly increase the likelihood of giving compared to a simple 1:1 match. This supports the authors’ conclusion in the original paper: once a match is offered, making it more generous (e.g., 2:1 or 3:1) doesn’t lead to higher donation rates. From a behavioral perspective, donors may simply respond to the existence of a match rather than its size.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ntreatment_df = df[df['treatment'] == 1].copy()\n\ntreatment_df['ratio1'] = (treatment_df['ratio'] == 1).astype(int)\ntreatment_df['ratio2'] = (treatment_df['ratio'] == 2).astype(int)\ntreatment_df['ratio3'] = (treatment_df['ratio'] == 3).astype(int)\n\nreg = smf.ols('gave ~ ratio2 + ratio3', data=treatment_df).fit()\n\nresp_1to1 = treatment_df[treatment_df['ratio'] == 1]['gave'].mean()\nresp_2to1 = treatment_df[treatment_df['ratio'] == 2]['gave'].mean()\nresp_3to1 = treatment_df[treatment_df['ratio'] == 3]['gave'].mean()\n\nraw_diff_2v1 = resp_2to1 - resp_1to1\nraw_diff_3v2 = resp_3to1 - resp_2to1\n\ncoef_2to1 = reg.params['ratio2']\ncoef_3to1 = reg.params['ratio3']\nfitted_diff_2v1 = coef_2to1\nfitted_diff_3v2 = coef_3to1 - coef_2to1\n\nsummary_table = pd.DataFrame({\n    \"Comparison\": [\n        \"Raw: 2:1 vs 1:1\",\n        \"Raw: 3:1 vs 2:1\",\n        \"Fitted: 2:1 vs 1:1\",\n        \"Fitted: 3:1 vs 2:1\"\n    ],\n    \"Difference\": [\n        round(raw_diff_2v1, 4),\n        round(raw_diff_3v2, 4),\n        round(fitted_diff_2v1, 4),\n        round(fitted_diff_3v2, 4)\n    ]\n})\n\nsummary_table\n\n\n\n\n\n\n\n\nComparison\nDifference\n\n\n\n\n0\nRaw: 2:1 vs 1:1\n0.0019\n\n\n1\nRaw: 3:1 vs 2:1\n0.0001\n\n\n2\nFitted: 2:1 vs 1:1\n0.0019\n\n\n3\nFitted: 3:1 vs 2:1\n0.0001\n\n\n\n\n\n\n\nTo assess whether more generous match ratios (2:1 or 3:1) encourage higher donation response rates, we compared both the raw data and regression-based estimates for donation behavior among individuals in the treatment group.\nThe response rate increased by only 0.19 percentage points when moving from a 1:1 to a 2:1 match. The difference between 2:1 and 3:1 match ratios was even smaller—just 0.01 percentage points.\nRegression results (from a model using ratio1 as the baseline) confirmed these findings. The fitted coefficient for the 2:1 group was 0.0019, and the difference between 3:1 and 2:1 was just 0.0001—both statistically insignificant.\nThese results lead to a clear conclusion: while matched donations increase response rates overall, increasing the match ratio beyond 1:1 does not provide additional benefit. Donors appear to be motivated by the idea of matching itself, but not by the generosity of the match. This supports Karlan and List’s original finding that the existence of a match is more influential than the size of the match when it comes to motivating charitable behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\n# Limit to people who donated\ndonors_df = df[df['amount'] &gt; 0].copy()\n\n# T-test: donation amount among donors\nt_stat_donors, p_val_donors = ttest_ind(\n    donors_df[donors_df['treatment'] == 1]['amount'],\n    donors_df[donors_df['treatment'] == 0]['amount'],\n    equal_var=False\n)\n\n# Linear regression: amount ~ treatment (for donors)\nreg_donors = smf.ols('amount ~ treatment', data=donors_df).fit()\n\n# Format as summary table\nsummary_donors = pd.DataFrame({\n    \"Method\": [\"T-test\", \"OLS Regression\"],\n    \"Estimate\": [None, round(reg_donors.params['treatment'], 4)],\n    \"t-statistic\": [round(t_stat_donors, 4), round(reg_donors.tvalues['treatment'], 4)],\n    \"p-value\": [round(p_val_donors, 4), round(reg_donors.pvalues['treatment'], 4)],\n    \"R-squared\": [None, round(reg_donors.rsquared, 4)]\n})\n\nsummary_donors\n\n\n\n\n\n\n\n\nMethod\nEstimate\nt-statistic\np-value\nR-squared\n\n\n\n\n0\nT-test\nNaN\n-0.5846\n0.5590\nNaN\n\n\n1\nOLS Regression\n-1.6684\n-0.5808\n0.5615\n0.0003\n\n\n\n\n\n\n\nTo evaluate whether offering a matched donation affects the size of the donation, We compared the donation amounts between the treatment and control groups using both a t-test and a linear regression.\nThe t-test revealed a marginal difference in donation amounts, with a p-value of 0.055, slightly above the conventional 0.05 threshold for statistical significance. Similarly, the regression model estimated that donors in the treatment group gave an average of $0.15 more than those in the control group, with a p-value of 0.063.\nThese results suggest that the matched donation treatment may have a small positive effect on how much people give, but the evidence is not strong enough to be statistically conclusive. In conclusion, while matched donations appear effective at encouraging people to donate, they do not meaningfully increase the average amount donated. Their primary value lies in boosting participation, not in raising contribution size.\n\ndonors_df = df[df['amount'] &gt; 0].copy()\n\nt_stat_donors, p_val_donors = ttest_ind(\n    donors_df[donors_df['treatment'] == 1]['amount'],\n    donors_df[donors_df['treatment'] == 0]['amount'],\n    equal_var=False\n)\n\nreg_donors = smf.ols('amount ~ treatment', data=donors_df).fit()\n\nsummary_donors = pd.DataFrame({\n    \"Method\": [\"T-test\", \"OLS Regression\"],\n    \"Estimate\": [None, round(reg_donors.params['treatment'], 4)],\n    \"t-statistic\": [round(t_stat_donors, 4), round(reg_donors.tvalues['treatment'], 4)],\n    \"p-value\": [round(p_val_donors, 4), round(reg_donors.pvalues['treatment'], 4)],\n    \"R-squared\": [None, round(reg_donors.rsquared, 4)]\n})\n\nsummary_donors\n\n\n\n\n\n\n\n\nMethod\nEstimate\nt-statistic\np-value\nR-squared\n\n\n\n\n0\nT-test\nNaN\n-0.5846\n0.5590\nNaN\n\n\n1\nOLS Regression\n-1.6684\n-0.5808\n0.5615\n0.0003\n\n\n\n\n\n\n\nTo analyze whether matched donations influenced how much people donated, we restricted the data to individuals who made a donation and compared donation sizes between treatment and control groups.\nThe results of t-test, indicating no statistically significant difference in average donation amounts between the two groups. Similarly, the linear regression showed that donors in the treatment group gave $1.67 less on average, but this difference was also not statistically significant.\nThese results suggest that while the matched donation offer increased the likelihood of giving, it did not lead to higher donations among those who gave. Moreover, because this analysis is conditional on donating—a behavior influenced by the treatment—the regression coefficient cannot be interpreted causally due to potential selection bias.\n\ndonors_df = df[df['amount'] &gt; 0]\n\ndonors_treat = donors_df[donors_df['treatment'] == 1]['amount']\ndonors_ctrl = donors_df[donors_df['treatment'] == 0]['amount']\n\nmean_treat = donors_treat.mean()\nmean_ctrl = donors_ctrl.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\naxes[0].hist(donors_treat, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Treatment Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend([f'Mean = ${mean_treat:.2f}'], loc='upper right')\n\naxes[1].hist(donors_ctrl, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_ctrl, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Control Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend([f'Mean = ${mean_ctrl:.2f}'], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms show that while matched donation offers increase the likelihood of giving, they do not increase the amount donated. Both treatment and control groups exhibit similar right-skewed distributions, with the treatment group’s average donation slightly lower. This confirms earlier findings that matching influences participation more than contribution size."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\ncontrol_draws = np.random.binomial(1, 0.018, 100000)\n\ntreatment_draws = np.random.binomial(1, 0.022, 10000)\n\ncontrol_sample = control_draws[:10000]\n\ndifferences = treatment_draws - control_sample\n\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average (Treatment - Control)', color='blue')\nplt.axhline(y=0.004, color='red', linestyle='--', label='True Mean Difference (0.004)')\nplt.title(\"Cumulative Average of Simulated Differences (Law of Large Numbers)\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis simulation demonstrates the Law of Large Numbers by plotting the cumulative average of 10,000 differences in donation outcomes between simulated treatment and control groups. The treatment group has a true donation probability of 2.2%, while the control group has 1.8%, implying a true difference of 0.004. Although the early cumulative averages fluctuate due to random variation, the plot clearly shows that the average stabilizes and converges toward the true difference as the number of observations increases. This visual evidence reinforces the statistical principle that, with large enough sample sizes, sample averages become reliable estimates of population parameters.\n\n\nCentral Limit Theorem\n\nnp.random.seed(42)\n\ncontrol_p = 0.018\ntreat_p = 0.022\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(num_simulations):\n        control_sample = np.random.binomial(1, control_p, n)\n        treatment_sample = np.random.binomial(1, treat_p, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        differences.append(diff)\n\n    ax = axes[i]\n    ax.hist(differences, bins=30, color='skyblue', edgecolor='black')\n    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n    ax.axvline(x=0.004, color='green', linestyle='--', linewidth=2, label='True Diff')\n    ax.set_title(f'Sample Size = {n}')\n    ax.set_xlabel('Avg. Difference (Treatment - Control)')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis simulation illustrates how sample size affects the distribution of estimated treatment effects. For each of four sample sizes (50, 200, 500, and 1000), we repeatedly drew samples from control and treatment groups and calculated the average difference in donation rates. At small sample sizes, the distributions are wide and irregular, with zero often appearing near the center—suggesting high variability and low power to detect small effects. As the sample size increases, the distributions become narrower and more centered around the true effect (0.004), and zero shifts toward the tail. This confirms the Central Limit Theorem and underscores the importance of large samples."
  },
  {
    "objectID": "Untitled-1.html",
    "href": "Untitled-1.html",
    "title": "1v0site",
    "section": "",
    "text": "pip install pandas\n\nDefaulting to user installation because normal site-packages is not writeable\nCollecting pandas\n  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n     |████████████████████████████████| 11.3 MB 7.4 MB/s eta 0:00:01\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\nCollecting numpy&gt;=1.22.4\n  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n     |████████████████████████████████| 5.3 MB 68.4 MB/s eta 0:00:01\nCollecting tzdata&gt;=2022.7\n  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n     |████████████████████████████████| 347 kB 80.5 MB/s eta 0:00:01\nCollecting pytz&gt;=2020.1\n  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n     |████████████████████████████████| 509 kB 72.3 MB/s eta 0:00:01\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.15.0)\nInstalling collected packages: tzdata, pytz, numpy, pandas\nSuccessfully installed numpy-2.0.2 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\nWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\npip install scipy\n\nDefaulting to user installation because normal site-packages is not writeable\nCollecting scipy\n  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n     |████████████████████████████████| 30.3 MB 6.6 MB/s eta 0:00:01\nRequirement already satisfied: numpy&lt;2.3,&gt;=1.22.4 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from scipy) (2.0.2)\nInstalling collected packages: scipy\nSuccessfully installed scipy-1.13.1\nWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\npip install statsmodels\n\nDefaulting to user installation because normal site-packages is not writeable\nCollecting statsmodels\n  Downloading statsmodels-0.14.4-cp39-cp39-macosx_11_0_arm64.whl (9.9 MB)\n     |████████████████████████████████| 9.9 MB 6.3 MB/s eta 0:00:01\nRequirement already satisfied: numpy&lt;3,&gt;=1.22.3 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from statsmodels) (2.0.2)\nCollecting patsy&gt;=0.5.6\n  Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n     |████████████████████████████████| 232 kB 86.4 MB/s eta 0:00:01\nRequirement already satisfied: packaging&gt;=21.3 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from statsmodels) (24.1)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.8 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from statsmodels) (1.13.1)\nRequirement already satisfied: pandas!=2.1.0,&gt;=1.4 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from statsmodels) (2.2.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas!=2.1.0,&gt;=1.4-&gt;statsmodels) (1.15.0)\nInstalling collected packages: patsy, statsmodels\nSuccessfully installed patsy-1.0.1 statsmodels-0.14.4\nWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\ndef test_balance(variable: str):\n    # Drop missing values\n    treat = df[df['treatment'] == 1][variable].dropna()\n    ctrl = df[df['treatment'] == 0][variable].dropna()\n\n    X1, X2 = treat.mean(), ctrl.mean()\n    s1, s2 = treat.std(), ctrl.std()\n    n1, n2 = len(treat), len(ctrl)\n\n    # Manual t-statistic using class slide formula\n    t_stat_manual = (X1 - X2) / np.sqrt((s1**2)/n1 + (s2**2)/n2)\n\n    reg = smf.ols(f'{variable} ~ treatment', data=df[['treatment', variable]].dropna()).fit()\n    coef = reg.params['treatment']\n    t_stat_reg = reg.tvalues['treatment']\n    p_val = reg.pvalues['treatment']\n\n    return {\n        \"variable\": variable,\n        \"manual t-stat\": round(t_stat_manual, 4),\n        \"regression coef\": round(coef, 4),\n        \"regression t-stat\": round(t_stat_reg, 4),\n        \"p-value\": round(p_val, 4),\n        \"significant at 95%\": p_val &lt; 0.05\n    }\n\nall_vars_to_test = [\n    'mrm2', 'years', 'female', 'couple',     # behavioral/demographic\n    'hpa', 'ltmedmra', 'freq',               # prior donation history\n    'pwhite', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba'  # ZIP-level demographics\n]\n\nresults = [test_balance(var) for var in all_vars_to_test]\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\n\n\n\n\n\nvariable\nmanual t-stat\nregression coef\nregression t-stat\np-value\nsignificant at 95%\n\n\n\n\n0\nmrm2\n0.1195\n0.0137\n0.1195\n0.9049\nFalse\n\n\n1\nyears\n-1.0909\n-0.0575\n-1.1030\n0.2700\nFalse\n\n\n2\nfemale\n-1.7535\n-0.0075\n-1.7584\n0.0787\nFalse\n\n\n3\ncouple\n-0.5823\n-0.0016\n-0.5838\n0.5594\nFalse\n\n\n4\nhpa\n0.9704\n0.6371\n0.9441\n0.3451\nFalse\n\n\n5\nltmedmra\n1.9099\n0.0091\n1.9097\n0.0562\nFalse\n\n\n6\nfreq\n-0.1108\n-0.0120\n-0.1109\n0.9117\nFalse\n\n\n7\npwhite\n-0.5590\n-0.0009\n-0.5603\n0.5753\nFalse\n\n\n8\nave_hh_sz\n0.8234\n0.0030\n0.8243\n0.4098\nFalse\n\n\n9\nmedian_hhincome\n-0.7433\n-157.9255\n-0.7417\n0.4583\nFalse\n\n\n10\npowner\n0.1895\n0.0004\n0.1891\n0.8500\nFalse\n\n\n11\npsch_atlstba\n-1.8427\n-0.0033\n-1.8481\n0.0646\nFalse\n\n\n\n\n\n\n\n\npip install matplotlib\n\nDefaulting to user installation because normal site-packages is not writeable\nCollecting matplotlib\n  Downloading matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n     |████████████████████████████████| 7.8 MB 7.0 MB/s eta 0:00:01\nRequirement already satisfied: numpy&gt;=1.23 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.0.2)\nCollecting kiwisolver&gt;=1.3.1\n  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n     |████████████████████████████████| 64 kB 13.0 MB/s eta 0:00:01\nCollecting importlib-resources&gt;=3.2.0\n  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\nCollecting pillow&gt;=8\n  Downloading pillow-11.2.1-cp39-cp39-macosx_11_0_arm64.whl (3.0 MB)\n     |████████████████████████████████| 3.0 MB 35.6 MB/s eta 0:00:01\nRequirement already satisfied: packaging&gt;=20.0 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.1)\nCollecting cycler&gt;=0.10\n  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\nCollecting pyparsing&gt;=2.3.1\n  Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n     |████████████████████████████████| 111 kB 56.3 MB/s eta 0:00:01\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\nCollecting fonttools&gt;=4.22.0\n  Downloading fonttools-4.57.0-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\n     |████████████████████████████████| 2.8 MB 46.6 MB/s eta 0:00:01\nCollecting contourpy&gt;=1.0.1\n  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n     |████████████████████████████████| 249 kB 33.7 MB/s eta 0:00:01\nRequirement already satisfied: zipp&gt;=3.1.0 in /Users/1v0/Library/Python/3.9/lib/python/site-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib) (3.20.2)\nRequirement already satisfied: six&gt;=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.15.0)\nInstalling collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\nSuccessfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.57.0 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pillow-11.2.1 pyparsing-3.2.3\nWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\nYou should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport matplotlib.pyplot as plt\n\ndonation_rate = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\ndonation_rate.plot(kind='bar', edgecolor='black')\nplt.title(\"Proportion of People Who Donated by Group\")\nplt.ylabel(\"Donation Rate\")\nplt.xticks([0, 1], ['Control', 'Treatment'], rotation=0)\nplt.ylim(0, donation_rate.max() + 0.02)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_ctrl = df[df['treatment'] == 0]['gave']\n\nt_stat, p_val = ttest_ind(gave_treat, gave_ctrl, equal_var=False)\n\n# Linear regression: gave ~ treatment\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\n{\n    \"t-test\": {\n        \"t-statistic\": round(t_stat, 4),\n        \"p-value\": round(p_val, 4)\n    },\n    \"regression\": {\n        \"coef on treatment\": round(reg_gave.params['treatment'], 4),\n        \"t-statistic\": round(reg_gave.tvalues['treatment'], 4),\n        \"p-value\": round(reg_gave.pvalues['treatment'], 4),\n        \"R-squared\": round(reg_gave.rsquared, 4)\n    }\n}\n\n{'t-test': {'t-statistic': np.float64(3.2095), 'p-value': np.float64(0.0013)},\n 'regression': {'coef on treatment': np.float64(0.0042),\n  't-statistic': np.float64(3.1014),\n  'p-value': np.float64(0.0019),\n  'R-squared': np.float64(0.0002)}}\n\n\n\nimport statsmodels.api as sm\n\n# Run probit regression: gave ~ treatment\nprobit_model = sm.Probit(df['gave'], sm.add_constant(df['treatment']))\nprobit_result = probit_model.fit(disp=0)  # disp=0 suppresses output\n\n# Extract and format results\nsummary_table = {\n    \"coef (treatment)\": round(probit_result.params['treatment'], 4),\n    \"std error\": round(probit_result.bse['treatment'], 4),\n    \"z-stat\": round(probit_result.tvalues['treatment'], 4),\n    \"p-value\": round(probit_result.pvalues['treatment'], 4),\n    \"pseudo R-squared\": round(probit_result.prsquared, 4),\n    \"n obs\": int(probit_result.nobs)\n}\nsummary_table\n\n{'coef (treatment)': np.float64(0.0868),\n 'std error': np.float64(0.0279),\n 'z-stat': np.float64(3.1129),\n 'p-value': np.float64(0.0019),\n 'pseudo R-squared': np.float64(0.001),\n 'n obs': 50083}\n\n\n\n```{python}\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\ntreat_df = df[df['treatment'] == 1]\n\ngave_1to1 = treat_df[treat_df['ratio'] == 1]['gave']\ngave_2to1 = treat_df[treat_df['ratio'] == 2]['gave']\ngave_3to1 = treat_df[treat_df['ratio'] == 3]['gave']\n\nttest_2v1 = ttest_ind(gave_2to1, gave_1to1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3to1, gave_1to1, equal_var=False)\nttest_3v2 = ttest_ind(gave_3to1, gave_2to1, equal_var=False)\n\nt_test_results = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 1:1\", \"3:1 vs 2:1\"],\n    \"t-statistic\": [\n        round(ttest_2v1.statistic, 4),\n        round(ttest_3v1.statistic, 4),\n        round(ttest_3v2.statistic, 4)\n    ],\n    \"p-value\": [\n        round(ttest_2v1.pvalue, 4),\n        round(ttest_3v1.pvalue, 4),\n        round(ttest_3v2.pvalue, 4)\n    ]\n})\n\nt_test_results\n```\n\n{'2:1 vs 1:1': {'t-stat': np.float16(nan), 'p-value': np.float16(nan)},\n '3:1 vs 1:1': {'t-stat': np.float16(nan), 'p-value': np.float16(nan)},\n '3:1 vs 2:1': {'t-stat': np.float16(nan), 'p-value': np.float16(nan)}}\n\n\n\n# Corrected groupings based on numeric match ratio values\ngave_1to1 = treat_df[treat_df['ratio'] == 1]['gave']\ngave_2to1 = treat_df[treat_df['ratio'] == 2]['gave']\ngave_3to1 = treat_df[treat_df['ratio'] == 3]['gave']\n\nttest_2v1 = ttest_ind(gave_2to1, gave_1to1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3to1, gave_1to1, equal_var=False)\nttest_3v2 = ttest_ind(gave_3to1, gave_2to1, equal_var=False)\n\n{\n    \"2:1 vs 1:1\": {\"t-stat\": round(ttest_2v1.statistic, 4), \"p-value\": round(ttest_2v1.pvalue, 4)},\n    \"3:1 vs 1:1\": {\"t-stat\": round(ttest_3v1.statistic, 4), \"p-value\": round(ttest_3v1.pvalue, 4)},\n    \"3:1 vs 2:1\": {\"t-stat\": round(ttest_3v2.statistic, 4), \"p-value\": round(ttest_3v2.pvalue, 4)}\n}\n\n{'2:1 vs 1:1': {'t-stat': np.float64(0.965), 'p-value': np.float64(0.3345)},\n '3:1 vs 1:1': {'t-stat': np.float64(1.015), 'p-value': np.float64(0.3101)},\n '3:1 vs 2:1': {'t-stat': np.float64(0.0501), 'p-value': np.float64(0.96)}}\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# Step 1: Subset the treatment group only\ntreatment_df = df[df['treatment'] == 1].copy()\n\n# Step 2: Create dummy variables for each match ratio\n# (1:1, 2:1, 3:1) — create ratio1 just to follow instruction, though it will be omitted in regression\ntreatment_df['ratio1'] = (treatment_df['ratio'] == 1).astype(int)\ntreatment_df['ratio2'] = (treatment_df['ratio'] == 2).astype(int)\ntreatment_df['ratio3'] = (treatment_df['ratio'] == 3).astype(int)\n\n# Step 3: Run regression using ratio1 as the baseline\nreg = smf.ols('gave ~ ratio2 + ratio3', data=treatment_df).fit()\nprint(\"=== Regression: gave ~ ratio2 + ratio3 (baseline is ratio1) ===\")\nprint(reg.summary())\n\n# Optional Step 4: Categorical regression (not recommended here due to multicollinearity issues)\n# Works best if ratio is treated as a category and one level is dropped automatically\nreg_cat = smf.ols('gave ~ C(ratio)', data=treatment_df).fit()\nprint(\"\\n=== Categorical Regression: gave ~ C(ratio) ===\")\nprint(reg_cat.summary())\n\n=== Regression: gave ~ ratio2 + ratio3 (baseline is ratio1) ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Fri, 18 Apr 2025   Prob (F-statistic):              0.524\nTime:                        14:39:48   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n=== Categorical Regression: gave ~ C(ratio) ===\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.4287\nDate:                Fri, 18 Apr 2025   Prob (F-statistic):              0.732\nTime:                        14:39:48   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33392   BIC:                        -3.333e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept      1.618e+09   2.38e+10      0.068      0.946    -4.5e+10    4.82e+10\nC(ratio)[T.1] -1.618e+09   2.38e+10     -0.068      0.946   -4.82e+10     4.5e+10\nC(ratio)[T.2] -1.618e+09   2.38e+10     -0.068      0.946   -4.82e+10     4.5e+10\nC(ratio)[T.3] -1.618e+09   2.38e+10     -0.068      0.946   -4.82e+10     4.5e+10\n==============================================================================\nOmnibus:                    38964.103   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506517.682\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.395   Cond. No.                     6.83e+13\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 9.54e-24. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n\ntreatment_df = df[df['treatment'] == 1].copy()\n\n# Create dummy variables\ntreatment_df['ratio1'] = (treatment_df['ratio'] == 1).astype(int)\ntreatment_df['ratio2'] = (treatment_df['ratio'] == 2).astype(int)\ntreatment_df['ratio3'] = (treatment_df['ratio'] == 3).astype(int)\n\n# Fit the regression again\nreg = smf.ols('gave ~ ratio2 + ratio3', data=treatment_df).fit()\n\n# Calculate raw response rate differences directly from the data\nresp_1to1 = treatment_df[treatment_df['ratio'] == 1]['gave'].mean()\nresp_2to1 = treatment_df[treatment_df['ratio'] == 2]['gave'].mean()\nresp_3to1 = treatment_df[treatment_df['ratio'] == 3]['gave'].mean()\n\nraw_diff_2v1 = resp_2to1 - resp_1to1\nraw_diff_3v2 = resp_3to1 - resp_2to1\n\n# Get fitted coefficient differences from regression (baseline is 1:1)\ncoef_2to1 = reg.params['ratio2']\ncoef_3to1 = reg.params['ratio3']\nfitted_diff_2v1 = coef_2to1\nfitted_diff_3v2 = coef_3to1 - coef_2to1\n\n{\n    \"Raw response rate difference (2:1 - 1:1)\": round(raw_diff_2v1, 4),\n    \"Raw response rate difference (3:1 - 2:1)\": round(raw_diff_3v2, 4),\n    \"Fitted coefficient difference (2:1 - 1:1)\": round(fitted_diff_2v1, 4),\n    \"Fitted coefficient difference (3:1 - 2:1)\": round(fitted_diff_3v2, 4)\n}\n\n{'Raw response rate difference (2:1 - 1:1)': np.float64(0.0019),\n 'Raw response rate difference (3:1 - 2:1)': np.float64(0.0001),\n 'Fitted coefficient difference (2:1 - 1:1)': np.float64(0.0019),\n 'Fitted coefficient difference (3:1 - 2:1)': np.float64(0.0001)}\n\n\n\nfrom scipy.stats import ttest_ind\n\n# Subset donation amount data by treatment status\namount_treat = df[df['treatment'] == 1]['amount']\namount_ctrl = df[df['treatment'] == 0]['amount']\n\n# T-test: donation amount by treatment vs. control\nt_stat_amt, p_val_amt = ttest_ind(amount_treat, amount_ctrl, equal_var=False)\n\n# Linear regression: amount ~ treatment\nreg_amt = smf.ols('amount ~ treatment', data=df).fit()\n\n# Summary of findings\n{\n    \"t-test (amount)\": {\n        \"t-statistic\": round(t_stat_amt, 4),\n        \"p-value\": round(p_val_amt, 4)\n    },\n    \"regression\": {\n        \"coef on treatment\": round(reg_amt.params['treatment'], 4),\n        \"t-statistic\": round(reg_amt.tvalues['treatment'], 4),\n        \"p-value\": round(reg_amt.pvalues['treatment'], 4),\n        \"R-squared\": round(reg_amt.rsquared, 4)\n    }\n}\n\n{'t-test (amount)': {'t-statistic': np.float64(1.9183),\n  'p-value': np.float64(0.0551)},\n 'regression': {'coef on treatment': np.float64(0.1536),\n  't-statistic': np.float64(1.8605),\n  'p-value': np.float64(0.0628),\n  'R-squared': np.float64(0.0001)}}\n\n\n\n# Limit data to people who made a donation (amount &gt; 0)\ndonors_df = df[df['amount'] &gt; 0].copy()\n\n# T-test on donation amount for donors only\nt_stat_donors, p_val_donors = ttest_ind(\n    donors_df[donors_df['treatment'] == 1]['amount'],\n    donors_df[donors_df['treatment'] == 0]['amount'],\n    equal_var=False\n)\n\n# Linear regression on amount for donors only\nreg_donors = smf.ols('amount ~ treatment', data=donors_df).fit()\n\n# Summary of findings\n{\n    \"t-test (donors only)\": {\n        \"t-statistic\": round(t_stat_donors, 4),\n        \"p-value\": round(p_val_donors, 4)\n    },\n    \"regression (donors only)\": {\n        \"coef on treatment\": round(reg_donors.params['treatment'], 4),\n        \"t-statistic\": round(reg_donors.tvalues['treatment'], 4),\n        \"p-value\": round(reg_donors.pvalues['treatment'], 4),\n        \"R-squared\": round(reg_donors.rsquared, 4)\n    }\n}\n\n{'t-test (donors only)': {'t-statistic': np.float64(-0.5846),\n  'p-value': np.float64(0.559)},\n 'regression (donors only)': {'coef on treatment': np.float64(-1.6684),\n  't-statistic': np.float64(-0.5808),\n  'p-value': np.float64(0.5615),\n  'R-squared': np.float64(0.0003)}}\n\n\n\nimport matplotlib.pyplot as plt\n\n# Subset to only donors (amount &gt; 0)\ndonors_df = df[df['amount'] &gt; 0]\n\n# Separate treatment and control donor groups\ndonors_treat = donors_df[donors_df['treatment'] == 1]['amount']\ndonors_ctrl = donors_df[donors_df['treatment'] == 0]['amount']\n\n# Calculate sample means\nmean_treat = donors_treat.mean()\nmean_ctrl = donors_ctrl.mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Treatment group plot\naxes[0].hist(donors_treat, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Treatment Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend([f'Mean = ${mean_treat:.2f}'], loc='upper right')\n\n# Control group plot\naxes[1].hist(donors_ctrl, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_ctrl, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Control Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend([f'Mean = ${mean_ctrl:.2f}'], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport scipy.stats as stats\n\n# Set up values\nx = np.linspace(-4, 4, 1000)\nt_dist = stats.t(df=30)  # example df = 30\nt_stat_example = -2.6\np_value = 2 * t_dist.cdf(t_stat_example)  # two-tailed p-value for t = -2.6\n\n# Plot the t-distribution\nplt.figure(figsize=(10, 5))\nplt.plot(x, t_dist.pdf(x), color='black', linewidth=2, label='t-distribution (df=30)')\n\n# Shade the two tails\nx_tail_left = np.linspace(-4, t_stat_example, 300)\nx_tail_right = np.linspace(-t_stat_example, 4, 300)\nplt.fill_between(x_tail_left, t_dist.pdf(x_tail_left), color='purple', alpha=0.4)\nplt.fill_between(x_tail_right, t_dist.pdf(x_tail_right), color='purple', alpha=0.4)\n\n# Vertical line for t-stat\nplt.axvline(x=t_stat_example, color='green', linestyle='--', label=f't = {t_stat_example} (p ≈ {p_value:.3f})')\nplt.axvline(x=-t_stat_example, color='green', linestyle='--')\n\n# Labels and styling\nplt.title('T-distribution with Two-tailed p-value Example')\nplt.xlabel('t-statistic')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/hw1_questions.html",
    "href": "blog/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe control group received a standard letter, while the treatment groups received letters offering matching grants with varying features. These included different match ratios (1:1, 2:1, and 3:1), varying maximum matching amounts ($25,000, $50,000, $100,000, or unspecified), and suggested donation levels tied to past giving. The study aimed to isolate whether the “price” of giving—lowered through matching—would increase donation likelihood or size. Results showed that simply including a matching grant significantly increased response rates and revenue per letter. However, larger match ratios offered no additional benefit. The experiment provided strong empirical insight into donor behavior and the psychology behind charitable decision-making.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1_questions.html#introduction",
    "href": "blog/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe control group received a standard letter, while the treatment groups received letters offering matching grants with varying features. These included different match ratios (1:1, 2:1, and 3:1), varying maximum matching amounts ($25,000, $50,000, $100,000, or unspecified), and suggested donation levels tied to past giving. The study aimed to isolate whether the “price” of giving—lowered through matching—would increase donation likelihood or size. Results showed that simply including a matching grant significantly increased response rates and revenue per letter. However, larger match ratios offered no additional benefit. The experiment provided strong empirical insight into donor behavior and the psychology behind charitable decision-making.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/hw1_questions.html#data",
    "href": "blog/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\nThe dataset contains 50,083 observations and 51 variables from a natural field experiment on charitable giving. Each row represents a prior donor who received a fundraising letter with randomized treatments: a control letter or one of several matching grant variations. Key variables include match ratio (ratio), maximum match size (size), suggested donation amounts (ask), and donation behavior (gave, amount). Demographic and geographic data—like gender, income, urban status, and political affiliation—are also included. The dataset enables analysis of how different fundraising strategies affect donation likelihood and size, offering insights into behavioral economics and nonprofit fundraising effectiveness.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport numpy as np\nimport statsmodels.formula.api as smf\n\ndef test_balance(variable: str):\n    # Drop missing values\n    treat = df[df['treatment'] == 1][variable].dropna()\n    ctrl = df[df['treatment'] == 0][variable].dropna()\n\n    X1, X2 = treat.mean(), ctrl.mean()\n    s1, s2 = treat.std(), ctrl.std()\n    n1, n2 = len(treat), len(ctrl)\n\n    # Manual t-statistic using class slide formula\n    t_stat_manual = (X1 - X2) / np.sqrt((s1**2)/n1 + (s2**2)/n2)\n\n    reg = smf.ols(f'{variable} ~ treatment', data=df[['treatment', variable]].dropna()).fit()\n    coef = reg.params['treatment']\n    t_stat_reg = reg.tvalues['treatment']\n    p_val = reg.pvalues['treatment']\n\n    return {\n        \"variable\": variable,\n        \"manual t-stat\": round(t_stat_manual, 4),\n        \"regression coef\": round(coef, 4),\n        \"regression t-stat\": round(t_stat_reg, 4),\n        \"p-value\": round(p_val, 4),\n        \"significant at 95%\": p_val &lt; 0.05\n    }\n\nall_vars_to_test = [\n    'mrm2', 'years', 'female', 'couple',     # behavioral/demographic\n    'hpa', 'ltmedmra', 'freq',               # prior donation history\n    'pwhite', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba'  # ZIP-level demographics\n]\n\nresults = [test_balance(var) for var in all_vars_to_test]\n\nresults_df = pd.DataFrame(results)\nresults_df\n\n\n\n\n\n\n\n\nvariable\nmanual t-stat\nregression coef\nregression t-stat\np-value\nsignificant at 95%\n\n\n\n\n0\nmrm2\n0.1195\n0.0137\n0.1195\n0.9049\nFalse\n\n\n1\nyears\n-1.0909\n-0.0575\n-1.1030\n0.2700\nFalse\n\n\n2\nfemale\n-1.7535\n-0.0075\n-1.7584\n0.0787\nFalse\n\n\n3\ncouple\n-0.5823\n-0.0016\n-0.5838\n0.5594\nFalse\n\n\n4\nhpa\n0.9704\n0.6371\n0.9441\n0.3451\nFalse\n\n\n5\nltmedmra\n1.9099\n0.0091\n1.9097\n0.0562\nFalse\n\n\n6\nfreq\n-0.1108\n-0.0120\n-0.1109\n0.9117\nFalse\n\n\n7\npwhite\n-0.5590\n-0.0009\n-0.5603\n0.5753\nFalse\n\n\n8\nave_hh_sz\n0.8234\n0.0030\n0.8243\n0.4098\nFalse\n\n\n9\nmedian_hhincome\n-0.7433\n-157.9255\n-0.7417\n0.4583\nFalse\n\n\n10\npowner\n0.1895\n0.0004\n0.1891\n0.8500\nFalse\n\n\n11\npsch_atlstba\n-1.8427\n-0.0033\n-1.8481\n0.0646\nFalse\n\n\n\n\n\n\n\nWe conducted a series of balance tests on baseline covariates using both manual t-tests and linear regression. The t-tests follow the classical formula, where the difference in sample means between the treatment and control groups is scaled by the standard error of that difference. This standardization produces a t-statistic, which tells us how extreme the observed difference is under the assumption that there is no true difference.\nFor each variable tested—such as months since last donation (mrm2), years since initial donation (years), whether the last gift was below the median (ltmedmra), and demographic variables like gender (female)—we calculated the t-statistic, the corresponding p-value, and the regression-based estimate of group differences.Across all variables, the p-values exceeded 0.05, indicating no statistically significant differences between the treatment and control groups at the 95% confidence level. This strongly suggests that the random assignment was successful, and that the groups are balanced on observed characteristics.\nThis aligns with Table 1 in the original paper, which serves to reassure readers that the treatment effects observed later in the study can be attributed to the intervention itself, not to any pre-existing differences between groups. These balance checks are crucial in experimental work because they support the assumption that any observed differences in outcomes are causal, not confounded by selection bias or unbalanced covariates. In short, the t-test results confirm that the randomization worked as intended and that the internal validity of the study is strong."
  },
  {
    "objectID": "blog/hw1_questions.html#experimental-results",
    "href": "blog/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\ndonation_rate = df.groupby(\"treatment\")[\"gave\"].mean().rename({0: \"Control\", 1: \"Treatment\"})\n\nplt.figure(figsize=(6, 4))\ndonation_rate.plot(kind='bar', edgecolor='black')\nplt.title(\"Proportion of People Who Donated by Group\")\nplt.ylabel(\"Donation Rate\")\nplt.xticks([0, 1], ['Control', 'Treatment'], rotation=0)\nplt.ylim(0, donation_rate.max() + 0.02)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_ctrl = df[df['treatment'] == 0]['gave']\n\nt_stat, p_val = ttest_ind(gave_treat, gave_ctrl, equal_var=False)\n\nreg_gave = smf.ols('gave ~ treatment', data=df).fit()\n\nresults = pd.DataFrame({\n    \"Method\": [\"T-test\", \"OLS Regression\"],\n    \"Estimate\": [None, round(reg_gave.params['treatment'], 4)],\n    \"t-statistic\": [round(t_stat, 4), round(reg_gave.tvalues['treatment'], 4)],\n    \"p-value\": [round(p_val, 4), round(reg_gave.pvalues['treatment'], 4)],\n    \"R-squared\": [None, round(reg_gave.rsquared, 4)]\n})\n\nresults\n\n\n\n\n\n\n\n\nMethod\nEstimate\nt-statistic\np-value\nR-squared\n\n\n\n\n0\nT-test\nNaN\n3.2095\n0.0013\nNaN\n\n\n1\nOLS Regression\n0.0042\n3.1014\n0.0019\n0.0002\n\n\n\n\n\n\n\nThe results of both the t-test and regression show that individuals who received a matching donation offer were significantly more likely to donate than those in the control group. While the increase in donation rate is modest, the statistical evidence suggests it is unlikely due to chance. This highlights that even small behavioral nudges—like framing a donation as part of a matching gift—can meaningfully affect charitable behavior. People may perceive matched gifts as a way to increase their personal impact, which boosts their motivation to give. The findings support the idea that well-designed incentives can effectively increase donor participation.\n\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Run probit regression: gave ~ treatment\nprobit_model = sm.Probit(df['gave'], sm.add_constant(df['treatment']))\nprobit_result = probit_model.fit(disp=0)\n\n# Format result as a table\nprobit_table = pd.DataFrame({\n    \"Metric\": [\n        \"Coefficient (treatment)\", \n        \"Standard Error\", \n        \"z-stat\", \n        \"p-value\", \n        \"Pseudo R-squared\", \n        \"Number of Observations\"\n    ],\n    \"Value\": [\n        round(probit_result.params['treatment'], 4),\n        round(probit_result.bse['treatment'], 4),\n        round(probit_result.tvalues['treatment'], 4),\n        round(probit_result.pvalues['treatment'], 4),\n        round(probit_result.prsquared, 4),\n        int(probit_result.nobs)\n    ]\n})\n\nprobit_table\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nCoefficient (treatment)\n0.0868\n\n\n1\nStandard Error\n0.0279\n\n\n2\nz-stat\n3.1129\n\n\n3\np-value\n0.0019\n\n\n4\nPseudo R-squared\n0.0010\n\n\n5\nNumber of Observations\n50083.0000\n\n\n\n\n\n\n\nTo replicate Column 1 of Table 3 in Karlan and List (2007), we estimated a probit regression where the binary outcome variable is whether an individual made a charitable donation (gave), and the explanatory variable is assignment to the treatment group (treatment).\nThe regression output shows a statistically significant positive effect of the treatment on the probability of donating. The estimated coefficient for treatment is 0.087, with a standard error of 0.028, yielding a z-statistic of 3.11 and a p-value of 0.0019. The pseudo R-squared is 0.001, and the number of observations is 50,083—exactly matching the paper.\nThis result reinforces the paper’s main finding: matching offers causally increase the likelihood of donating. Even though the absolute change is small, it’s meaningful in the context of low baseline donation rates, and it highlights how subtle changes in framing can influence real-world giving behavior.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\ntreat_df = df[df['treatment'] == 1]\n\ngave_1to1 = treat_df[treat_df['ratio'] == 1]['gave']\ngave_2to1 = treat_df[treat_df['ratio'] == 2]['gave']\ngave_3to1 = treat_df[treat_df['ratio'] == 3]['gave']\n\nttest_2v1 = ttest_ind(gave_2to1, gave_1to1, equal_var=False)\nttest_3v1 = ttest_ind(gave_3to1, gave_1to1, equal_var=False)\nttest_3v2 = ttest_ind(gave_3to1, gave_2to1, equal_var=False)\n\nt_test_results = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 1:1\", \"3:1 vs 2:1\"],\n    \"t-statistic\": [\n        round(ttest_2v1.statistic, 4),\n        round(ttest_3v1.statistic, 4),\n        round(ttest_3v2.statistic, 4)\n    ],\n    \"p-value\": [\n        round(ttest_2v1.pvalue, 4),\n        round(ttest_3v1.pvalue, 4),\n        round(ttest_3v2.pvalue, 4)\n    ]\n})\n\nt_test_results\n\n\n\n\n\n\n\n\nComparison\nt-statistic\np-value\n\n\n\n\n0\n2:1 vs 1:1\n0.9650\n0.3345\n\n\n1\n3:1 vs 1:1\n1.0150\n0.3101\n\n\n2\n3:1 vs 2:1\n0.0501\n0.9600\n\n\n\n\n\n\n\nTo evaluate whether larger match ratios (2:1 or 3:1) increase the likelihood of donating compared to a 1:1 match, we conducted a series of t-tests using only individuals in the treatment group.\nThe results showed no statistically significant differences in donation rates between: * 2:1 and 1:1 match groups * 3:1 and 1:1 match groups * 3:1 and 2:1 match groups\nThese findings suggest that increasing the generosity of the match offer does not significantly affect whether people donate. This directly supports the authors’ comment in the paper (page 8), where they note that the match ratio had no additional effect on donor behavior beyond the presence of a match itself.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ntreatment_df = df[df['treatment'] == 1].copy()\n\ntreatment_df['ratio1'] = (treatment_df['ratio'] == 1).astype(int)\ntreatment_df['ratio2'] = (treatment_df['ratio'] == 2).astype(int)\ntreatment_df['ratio3'] = (treatment_df['ratio'] == 3).astype(int)\n\nreg = smf.ols('gave ~ ratio2 + ratio3', data=treatment_df).fit()\n\nols_table = pd.DataFrame({\n    \"Variable\": reg.params.index,\n    \"Coefficient\": reg.params.round(4).values,\n    \"Std. Error\": reg.bse.round(4).values,\n    \"t-stat\": reg.tvalues.round(4).values,\n    \"p-value\": reg.pvalues.round(4).values\n})\n\nols_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-stat\np-value\n\n\n\n\n0\nIntercept\n0.0207\n0.0014\n14.9122\n0.0000\n\n\n1\nratio2\n0.0019\n0.0020\n0.9576\n0.3383\n\n\n2\nratio3\n0.0020\n0.0020\n1.0083\n0.3133\n\n\n\n\n\n\n\nTo assess whether higher match ratios (2:1 or 3:1) are more effective than a 1:1 match at encouraging donations, we ran a linear regression using data from individuals in the treatment group only.\nThese results suggest that larger match ratios do not significantly increase the likelihood of giving compared to a simple 1:1 match. This supports the authors’ conclusion in the original paper: once a match is offered, making it more generous (e.g., 2:1 or 3:1) doesn’t lead to higher donation rates. From a behavioral perspective, donors may simply respond to the existence of a match rather than its size.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ntreatment_df = df[df['treatment'] == 1].copy()\n\ntreatment_df['ratio1'] = (treatment_df['ratio'] == 1).astype(int)\ntreatment_df['ratio2'] = (treatment_df['ratio'] == 2).astype(int)\ntreatment_df['ratio3'] = (treatment_df['ratio'] == 3).astype(int)\n\nreg = smf.ols('gave ~ ratio2 + ratio3', data=treatment_df).fit()\n\nresp_1to1 = treatment_df[treatment_df['ratio'] == 1]['gave'].mean()\nresp_2to1 = treatment_df[treatment_df['ratio'] == 2]['gave'].mean()\nresp_3to1 = treatment_df[treatment_df['ratio'] == 3]['gave'].mean()\n\nraw_diff_2v1 = resp_2to1 - resp_1to1\nraw_diff_3v2 = resp_3to1 - resp_2to1\n\ncoef_2to1 = reg.params['ratio2']\ncoef_3to1 = reg.params['ratio3']\nfitted_diff_2v1 = coef_2to1\nfitted_diff_3v2 = coef_3to1 - coef_2to1\n\nsummary_table = pd.DataFrame({\n    \"Comparison\": [\n        \"Raw: 2:1 vs 1:1\",\n        \"Raw: 3:1 vs 2:1\",\n        \"Fitted: 2:1 vs 1:1\",\n        \"Fitted: 3:1 vs 2:1\"\n    ],\n    \"Difference\": [\n        round(raw_diff_2v1, 4),\n        round(raw_diff_3v2, 4),\n        round(fitted_diff_2v1, 4),\n        round(fitted_diff_3v2, 4)\n    ]\n})\n\nsummary_table\n\n\n\n\n\n\n\n\nComparison\nDifference\n\n\n\n\n0\nRaw: 2:1 vs 1:1\n0.0019\n\n\n1\nRaw: 3:1 vs 2:1\n0.0001\n\n\n2\nFitted: 2:1 vs 1:1\n0.0019\n\n\n3\nFitted: 3:1 vs 2:1\n0.0001\n\n\n\n\n\n\n\nTo assess whether more generous match ratios (2:1 or 3:1) encourage higher donation response rates, we compared both the raw data and regression-based estimates for donation behavior among individuals in the treatment group.\nThe response rate increased by only 0.19 percentage points when moving from a 1:1 to a 2:1 match. The difference between 2:1 and 3:1 match ratios was even smaller—just 0.01 percentage points.\nRegression results (from a model using ratio1 as the baseline) confirmed these findings. The fitted coefficient for the 2:1 group was 0.0019, and the difference between 3:1 and 2:1 was just 0.0001—both statistically insignificant.\nThese results lead to a clear conclusion: while matched donations increase response rates overall, increasing the match ratio beyond 1:1 does not provide additional benefit. Donors appear to be motivated by the idea of matching itself, but not by the generosity of the match. This supports Karlan and List’s original finding that the existence of a match is more influential than the size of the match when it comes to motivating charitable behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\n# Limit to people who donated\ndonors_df = df[df['amount'] &gt; 0].copy()\n\n# T-test: donation amount among donors\nt_stat_donors, p_val_donors = ttest_ind(\n    donors_df[donors_df['treatment'] == 1]['amount'],\n    donors_df[donors_df['treatment'] == 0]['amount'],\n    equal_var=False\n)\n\n# Linear regression: amount ~ treatment (for donors)\nreg_donors = smf.ols('amount ~ treatment', data=donors_df).fit()\n\n# Format as summary table\nsummary_donors = pd.DataFrame({\n    \"Method\": [\"T-test\", \"OLS Regression\"],\n    \"Estimate\": [None, round(reg_donors.params['treatment'], 4)],\n    \"t-statistic\": [round(t_stat_donors, 4), round(reg_donors.tvalues['treatment'], 4)],\n    \"p-value\": [round(p_val_donors, 4), round(reg_donors.pvalues['treatment'], 4)],\n    \"R-squared\": [None, round(reg_donors.rsquared, 4)]\n})\n\nsummary_donors\n\n\n\n\n\n\n\n\nMethod\nEstimate\nt-statistic\np-value\nR-squared\n\n\n\n\n0\nT-test\nNaN\n-0.5846\n0.5590\nNaN\n\n\n1\nOLS Regression\n-1.6684\n-0.5808\n0.5615\n0.0003\n\n\n\n\n\n\n\nTo evaluate whether offering a matched donation affects the size of the donation, We compared the donation amounts between the treatment and control groups using both a t-test and a linear regression.\nThe t-test revealed a marginal difference in donation amounts, with a p-value of 0.055, slightly above the conventional 0.05 threshold for statistical significance. Similarly, the regression model estimated that donors in the treatment group gave an average of $0.15 more than those in the control group, with a p-value of 0.063.\nThese results suggest that the matched donation treatment may have a small positive effect on how much people give, but the evidence is not strong enough to be statistically conclusive. In conclusion, while matched donations appear effective at encouraging people to donate, they do not meaningfully increase the average amount donated. Their primary value lies in boosting participation, not in raising contribution size.\n\ndonors_df = df[df['amount'] &gt; 0].copy()\n\nt_stat_donors, p_val_donors = ttest_ind(\n    donors_df[donors_df['treatment'] == 1]['amount'],\n    donors_df[donors_df['treatment'] == 0]['amount'],\n    equal_var=False\n)\n\nreg_donors = smf.ols('amount ~ treatment', data=donors_df).fit()\n\nsummary_donors = pd.DataFrame({\n    \"Method\": [\"T-test\", \"OLS Regression\"],\n    \"Estimate\": [None, round(reg_donors.params['treatment'], 4)],\n    \"t-statistic\": [round(t_stat_donors, 4), round(reg_donors.tvalues['treatment'], 4)],\n    \"p-value\": [round(p_val_donors, 4), round(reg_donors.pvalues['treatment'], 4)],\n    \"R-squared\": [None, round(reg_donors.rsquared, 4)]\n})\n\nsummary_donors\n\n\n\n\n\n\n\n\nMethod\nEstimate\nt-statistic\np-value\nR-squared\n\n\n\n\n0\nT-test\nNaN\n-0.5846\n0.5590\nNaN\n\n\n1\nOLS Regression\n-1.6684\n-0.5808\n0.5615\n0.0003\n\n\n\n\n\n\n\nTo analyze whether matched donations influenced how much people donated, we restricted the data to individuals who made a donation and compared donation sizes between treatment and control groups.\nThe results of t-test, indicating no statistically significant difference in average donation amounts between the two groups. Similarly, the linear regression showed that donors in the treatment group gave $1.67 less on average, but this difference was also not statistically significant.\nThese results suggest that while the matched donation offer increased the likelihood of giving, it did not lead to higher donations among those who gave. Moreover, because this analysis is conditional on donating—a behavior influenced by the treatment—the regression coefficient cannot be interpreted causally due to potential selection bias.\n\ndonors_df = df[df['amount'] &gt; 0]\n\ndonors_treat = donors_df[donors_df['treatment'] == 1]['amount']\ndonors_ctrl = donors_df[donors_df['treatment'] == 0]['amount']\n\nmean_treat = donors_treat.mean()\nmean_ctrl = donors_ctrl.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\naxes[0].hist(donors_treat, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Treatment Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend([f'Mean = ${mean_treat:.2f}'], loc='upper right')\n\naxes[1].hist(donors_ctrl, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_ctrl, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Control Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount')\naxes[1].legend([f'Mean = ${mean_ctrl:.2f}'], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe histograms show that while matched donation offers increase the likelihood of giving, they do not increase the amount donated. Both treatment and control groups exhibit similar right-skewed distributions, with the treatment group’s average donation slightly lower. This confirms earlier findings that matching influences participation more than contribution size."
  },
  {
    "objectID": "blog/hw1_questions.html#simulation-experiment",
    "href": "blog/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\ncontrol_draws = np.random.binomial(1, 0.018, 100000)\n\ntreatment_draws = np.random.binomial(1, 0.022, 10000)\n\ncontrol_sample = control_draws[:10000]\n\ndifferences = treatment_draws - control_sample\n\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average (Treatment - Control)', color='blue')\nplt.axhline(y=0.004, color='red', linestyle='--', label='True Mean Difference (0.004)')\nplt.title(\"Cumulative Average of Simulated Differences (Law of Large Numbers)\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis simulation demonstrates the Law of Large Numbers by plotting the cumulative average of 10,000 differences in donation outcomes between simulated treatment and control groups. The treatment group has a true donation probability of 2.2%, while the control group has 1.8%, implying a true difference of 0.004. Although the early cumulative averages fluctuate due to random variation, the plot clearly shows that the average stabilizes and converges toward the true difference as the number of observations increases. This visual evidence reinforces the statistical principle that, with large enough sample sizes, sample averages become reliable estimates of population parameters.\n\n\nCentral Limit Theorem\n\nnp.random.seed(42)\n\ncontrol_p = 0.018\ntreat_p = 0.022\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(num_simulations):\n        control_sample = np.random.binomial(1, control_p, n)\n        treatment_sample = np.random.binomial(1, treat_p, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        differences.append(diff)\n\n    ax = axes[i]\n    ax.hist(differences, bins=30, color='skyblue', edgecolor='black')\n    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero')\n    ax.axvline(x=0.004, color='green', linestyle='--', linewidth=2, label='True Diff')\n    ax.set_title(f'Sample Size = {n}')\n    ax.set_xlabel('Avg. Difference (Treatment - Control)')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis simulation illustrates how sample size affects the distribution of estimated treatment effects. For each of four sample sizes (50, 200, 500, and 1000), we repeatedly drew samples from control and treatment groups and calculated the average difference in donation rates. At small sample sizes, the distributions are wide and irregular, with zero often appearing near the center—suggesting high variability and low power to detect small effects. As the sample size increases, the distributions become narrower and more centered around the true effect (0.004), and zero shifts toward the tail. This confirms the Central Limit Theorem and underscores the importance of large samples."
  }
]